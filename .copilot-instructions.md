# GitHub Copilot Custom Instructions

This file provides custom instructions for GitHub Copilot to follow Machine Learning best practices and coding standards for this AI/ML project.

## Project Context

- **Project Name:** sp_AI_MachineLearning
- **Domain:** Artificial Intelligence, Machine Learning, Data Science
- **Purpose:** Educational project focused on statistical analysis, machine learning algorithms, and AI applications
- **Technologies:** Python, Jupyter Notebooks, scikit-learn, TensorFlow/Keras, pandas, numpy, matplotlib

## Code Formatting & Style

### Indentation
- **Use 2 spaces for indentation** (not tabs or 4 spaces)
- Apply consistently across all Python files and Jupyter notebooks
- Example:
```python
def train_model(X_train, y_train):
  model = RandomForestClassifier(
    n_estimators=100,
    random_state=42
  )
  model.fit(X_train, y_train)
  return model
```

### General Python Style
- Follow PEP8 guidelines with 2-space indentation
- Use descriptive variable names (e.g., `X_train`, `y_pred`, `accuracy_score`)
- Maximum line length: 88 characters
- Use snake_case for variables and functions
- Use UPPER_CASE for constants
- Use double quotes for strings unless single quotes are needed for clarity


## Machine Learning Best Practices



### 1. Data Handling
```python
# Always set random seeds for reproducibility
np.random.seed(42)
random.seed(42)

# Use train/validation/test splits
X_train, X_temp, y_train, y_temp = train_test_split(
  X, y, test_size=0.4, random_state=42, stratify=y
)
X_val, X_test, y_val, y_test = train_test_split(
  X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp
)
```

### 2. Data Preprocessing
```python
# Always check for missing values and handle appropriately
print(f"Missing values: {df.isnull().sum().sum()}")

# Scale features when necessary
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)
```

### 3. Model Development
```python
# Use cross-validation for model evaluation
from sklearn.model_selection import cross_val_score
cv_scores = cross_val_score(
  model, X_train, y_train, cv=5, scoring='accuracy'
)
print(f"CV Accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})")

# Always evaluate on multiple metrics
from sklearn.metrics import classification_report, confusion_matrix
y_pred = model.predict(X_test)
print(classification_report(y_test, y_pred))
```

### 4. Model Evaluation & Validation
```python
# Plot learning curves to check for overfitting
def plot_learning_curve(model, X, y):
  train_sizes = np.linspace(0.1, 1.0, 10)
  train_scores = []
  val_scores = []

  for train_size in train_sizes:
    # Implementation with proper 2-space indentation
    pass
```

### 5. Hyperparameter Tuning
```python
# Use GridSearchCV or RandomizedSearchCV
from sklearn.model_selection import GridSearchCV

param_grid = {
  'n_estimators': [50, 100, 200],
  'max_depth': [3, 5, 7, None],
  'min_samples_split': [2, 5, 10]
}

grid_search = GridSearchCV(
  RandomForestClassifier(random_state=42),
  param_grid,
  cv=5,
  scoring='accuracy',
  n_jobs=-1
)
```

## Documentation Standards

### Function Documentation
```python
def evaluate_model(model, X_test, y_test, model_name="Model"):
  """
  Evaluate a trained model and print comprehensive metrics.

  Args:
    model: Trained scikit-learn model
    X_test (array-like): Test features
    y_test (array-like): True test labels
    model_name (str): Name of the model for display

  Returns:
    dict: Dictionary containing evaluation metrics
  """
  y_pred = model.predict(X_test)
  accuracy = accuracy_score(y_test, y_pred)

  print(f"\n{model_name} Performance:")
  print(f"Accuracy: {accuracy:.4f}")

  return {'accuracy': accuracy, 'predictions': y_pred}
```

### Jupyter Notebook Structure
```python
# Cell 1: Imports and Setup
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report

# Set random seed and plotting style
np.random.seed(42)
plt.style.use('seaborn-v0_8')

# Cell 2: Data Loading and Initial Exploration
# Cell 3: Data Preprocessing
# Cell 4: Exploratory Data Analysis
# Cell 5: Model Training
# Cell 6: Model Evaluation
# Cell 7: Results Visualization
```

## Visualization Guidelines

```python
# Always label plots properly with 2-space indentation
def create_confusion_matrix_plot(y_true, y_pred, class_names):
  """Create a well-formatted confusion matrix plot."""
  cm = confusion_matrix(y_true, y_pred)

  plt.figure(figsize=(8, 6))
  sns.heatmap(
    cm,
    annot=True,
    fmt='d',
    cmap='Blues',
    xticklabels=class_names,
    yticklabels=class_names
  )
  plt.title('Confusion Matrix', fontsize=14, fontweight='bold')
  plt.xlabel('Predicted Label', fontsize=12)
  plt.ylabel('True Label', fontsize=12)
  plt.tight_layout()
  plt.show()
```

## Error Handling & Validation

```python
# Always validate input data
def validate_data(X, y):
  """Validate input data for ML pipeline."""
  if X.shape[0] != y.shape[0]:
    raise ValueError("X and y must have the same number of samples")

  if X.isnull().any().any():
    print("Warning: Missing values detected in features")

  if len(np.unique(y)) < 2:
    raise ValueError("Target variable must have at least 2 classes")
```

## Performance & Memory Considerations

```python
# Use efficient data types
df = pd.read_csv('data.csv')
df = df.astype({
  'category_col': 'category',
  'int_col': 'int32',
  'float_col': 'float32'
})

# Monitor memory usage for large datasets
print(f"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB")
```

## Common ML Patterns to Follow

1. **Data Pipeline**: Load → Clean → Split → Scale → Train → Evaluate
2. **Always use stratified sampling** for classification problems
3. **Feature engineering** before model training
4. **Cross-validation** for model selection
5. **Proper evaluation metrics** based on problem type
6. **Visualization** of results and model performance
7. **Save trained models** with joblib or pickle

## Example Complete ML Workflow

```python
# 1. Data Loading and Preprocessing
def load_and_preprocess_data(filepath):
  df = pd.read_csv(filepath)
  # Data cleaning with 2-space indentation
  df = df.dropna()
  return df

# 2. Feature Engineering
def create_features(df):
  # Feature creation with proper indentation
  pass

# 3. Model Training Pipeline
def train_evaluate_model(X_train, X_test, y_train, y_test):
  models = {
    'Random Forest': RandomForestClassifier(random_state=42),
    'SVM': SVC(random_state=42),
    'Logistic Regression': LogisticRegression(random_state=42)
  }

  results = {}
  for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    results[name] = accuracy
    print(f"{name}: {accuracy:.4f}")

  return results
```

Remember: Always prioritize code readability, reproducibility, and proper documentation in all ML implementations.

This file is intended to provide custom instructions for GitHub Copilot to tailor its code suggestions and completions for this project.

## Project Context

- **Project Name:** sp_AI_MachineLearning
- **Domain:** Artificial Intelligence, Machine Learning
- **Purpose:** This project is focused on providing statistical analysis and machine learning solutions for course statistics.

## Coding Guidelines

1. **Language Preference:** Python is the primary language. Use libraries such as pandas, numpy, scikit-learn, matplotlib, and seaborn for data analysis and visualization.
2. **Code Style:** Follow PEP8 guidelines. Use clear variable names and concise comments.
3. **Documentation:** Include docstrings for all functions and classes.
4. **Reproducibility:** Ensure code is reproducible. Use random seeds where applicable.
5. **Data Handling:** Assume data is in CSV format unless specified otherwise.
6. **Visualization:** Use matplotlib or seaborn for plots. Label axes and provide titles.

## Example Tasks

- Load and preprocess course statistics data.
- Perform exploratory data analysis (EDA).
- Build and evaluate machine learning models (classification/regression).
- Visualize results and model performance.

## Example Code Snippet
